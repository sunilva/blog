{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Slow Host","text":"<p>A blog on distributed systems discussing various topics like:Consensus, replication algorithms, commit protocols, transactions, consistency models, time, formal verification etc.</p> <p>The two fundamental problems concerning distributed systems are concurrency &amp; failure. As far as failure is concerned a host need not necessarily crash just by being slow it can prevent consensus from being achieved. This is the famous FLP result (as it is impossible to distinguish between a slow host &amp; crashed host in an asynchronous system). Hence the name of this blog.</p>"},{"location":"#posts","title":"Posts","text":"<p>Understanding Paxos Using Three Simple Rules</p>"},{"location":"about/","title":"About","text":"<p>You can contact me at my gmail-id: sunil.v.angadi </p>"},{"location":"bookmarks/","title":"Bookmarks","text":"<p>&lt;&lt; WORK IN PROGRESS &gt;&gt;</p>"},{"location":"reading/","title":"Reading","text":"<p>&lt;&lt; WORK IN PROGRESS &gt;&gt;</p>"},{"location":"todo/","title":"Todo","text":"<ul> <li>commit</li> <li>cockroach-db-arch</li> <li>distributed-lock</li> </ul>"},{"location":"todo/#cap-theorem","title":"CAP Theorem","text":""},{"location":"todo/#flp","title":"FLP","text":""},{"location":"todo/#consistency","title":"Consistency","text":""},{"location":"todo/#time","title":"Time","text":"<ul> <li>There is No Now</li> <li>Lamport Timestamps</li> <li>Vector Clocks</li> <li>Liskov Paper</li> <li>Lynch Paper</li> <li>Hybrid logical clock</li> </ul>"},{"location":"todo/#paxos","title":"Paxos","text":"<ul> <li>Part Time Parliament</li> <li>Paxos Made simple </li> <li>Heidi Howard</li> <li>Multi-Paxos TLA+ spec: </li> <li>Reconfiguration<ul> <li>Paxos Made Moderately Complex</li> <li>Vertical Paxos</li> </ul> </li> <li>Paxos variants:</li> </ul>"},{"location":"todo/#replication","title":"Replication","text":"<ul> <li>View Stamp Replication</li> <li>Raft </li> <li>Zookeeper</li> </ul>"},{"location":"todo/#locking-service","title":"Locking Service","text":"<ul> <li>Chubby</li> <li>Zookeeper</li> <li>RSM<ul> <li>Schneider</li> </ul> </li> </ul>"},{"location":"todo/#transactions","title":"Transactions","text":"<ul> <li>2PC &amp; 3PC</li> <li>Paxos Commit</li> <li>Highly Available Transactions</li> <li>CockroachDB</li> <li>Critique of ANSI</li> </ul>"},{"location":"todo/#consistency-models","title":"Consistency Models","text":"<ul> <li>Holy Grail: </li> </ul>"},{"location":"todo/#failure-detectors","title":"Failure Detectors","text":"<ul> <li>Minimal synchronization</li> <li>Partial Synchrony</li> <li>Unreliable failure detectors for reliable distributed systems</li> <li>Weakest failure detector for consensus </li> </ul>"},{"location":"todo/#tla","title":"TLA+","text":"<ul> <li>HyperBook</li> <li>Specifying Systems</li> <li>Practical TLA+</li> <li>Lamport Video Lectures: </li> <li>Blog: </li> <li>Ron Pressler</li> <li>video: </li> <li>blog-post:</li> </ul>"},{"location":"todo/#systems","title":"Systems","text":"<ul> <li>DynamoDB</li> <li>BigTable</li> <li>Spanner</li> <li>CockroachDB</li> <li>Calvin</li> </ul>"},{"location":"todo/#books","title":"Books","text":"<ul> <li>Data Intensive Applications</li> <li>Reliable Distributed Systems</li> <li>Distributed Algorithms - Nancy Lynch</li> <li>Algorithms for concurrent objects</li> </ul> <p>=============================================</p>"},{"location":"todo/#blogs","title":"Blogs","text":"<ul> <li>Murat Demibras </li> <li>Henry Robinson</li> <li>Marc Brooker</li> </ul>"},{"location":"todo/#practice","title":"Practice","text":"<ul> <li>Amazon Builder Library</li> <li>Microsoft Cloud Patterns</li> </ul>"},{"location":"todo/#distributed-system-reading-lists","title":"Distributed System Reading Lists","text":"<ul> <li>Henry Robinson: </li> <li>Marc Brooker:</li> <li>Murat Demibras:</li> </ul>"},{"location":"todo/#tla_1","title":"TLA+:","text":""},{"location":"canon-sys/canon-dist-sys/","title":"A Canonical Consistent Distributed System","text":""},{"location":"canon-sys/canon-dist-sys/#introduction","title":"Introduction","text":""},{"location":"canon-sys/canon-dist-sys/#locking-service","title":"Locking Service","text":""},{"location":"canon-sys/canon-dist-sys/#replicated-state-machine","title":"Replicated State Machine","text":""},{"location":"canon-sys/canon-dist-sys/#consensus","title":"Consensus","text":""},{"location":"canon-sys/canon-dist-sys/#replication-algorithms","title":"Replication Algorithms","text":""},{"location":"canon-sys/canon-dist-sys/#distributed-transactions","title":"Distributed Transactions","text":""},{"location":"canon-sys/canon-dist-sys/#two-phase-commit","title":"Two Phase Commit","text":""},{"location":"canon-sys/canon-dist-sys/#three-phase-commit","title":"Three Phase Commit","text":""},{"location":"canon-sys/canon-dist-sys/#non-blocking-atomic-commit","title":"Non Blocking Atomic Commit","text":""},{"location":"canon-sys/canon-dist-sys/#references","title":"References","text":""},{"location":"compare/compare-vsr-raft/","title":"Comparing VSR And Raft Algorithms","text":""},{"location":"compare/compare-vsr-raft/#introduction","title":"Introduction","text":"<p>View Stamp Replication(VSR) &amp; Raft are replication algorithms for implementing fault replicated state machines(basis for building other distributed applications like storage, database etc). </p> <p>VSR was first published in  independent of Paxos, however for this discussion we will be considering VSR revisited.   <p>Both assume asynchronous model in which no assumptions are made w.r.t time, crash failure &amp; network in which messages can be  delivered out-of-order, lost, duplicated &amp; delayed for arbitrary time but delivery is guaranteed if retried indefinitely i.e network is reliable.  VSR doesn't need disk &amp; assumes failure independence of the hosts.</p> <p>Raft's design is aimed as Understandability.</p> VSR Raft Primary Leader Backup Follower View Term Epoch -NA- Replication Normal Election View Change"},{"location":"compare/compare-vsr-raft/#replication","title":"Replication","text":"<p>One of the hosts in the system is elected a leader which receives requests from clients &amp; replicates the  requests to other hosts called followers.</p> <p>Raft sends two entries viz. current &amp; previous while replicating entries to follower, foll Raft sends empty append entry requests as heart beats to follower.</p> <p>VSR sends a single entry to the backup replicas, if backup identifies it is missing entries  the it does a GET-STATE operation to fetch missing entries</p>"},{"location":"compare/compare-vsr-raft/#leader-election","title":"Leader election","text":""},{"location":"compare/compare-vsr-raft/#raft","title":"Raft","text":"<p>Raft uses voting mechanism to elect a new leader. A follower increments it's term, becomes a candidate  and requests a votes from the followers. If any candidate receives a quorum of votes then it transitions to it's  state to become leader for the new term. </p> <p> State diagram for leader election is shown below:   timeline view <p>Safety:      A quorum of votes are required to be a leader for a given term. </p> <pre><code>Replication requests carry the current term, if follower's term doesn't match the leader the request is rejected \nby the follower.  If election fails due to split votes then election is started for next term abandoning the \nterm for which election was in progress.\n</code></pre> <p>Liveness:     Election can if votes get's split across more than one candidate &amp; nobody receives a majority of the votes(quorum). </p> <pre><code>Raft uses randomization technique so as to avoid multiple followers initiating election at about the same time\nresulting in split votes.\n</code></pre>"},{"location":"compare/compare-vsr-raft/#vsr","title":"VSR","text":"<p>VSR uses a three phase protocol to elect a new primary, the safety condition being: \"No committed entry should be lost\" </p> <ul> <li>Phase-1(SVC):      A quorum of replicas agree to stop participating in replication process no new entries get added to logs &amp; commit     doesn't happen. </li> <li>Phase-2(DVC):      The new primary gets logs from a quorum of replicas, the new primary determines log to be used for initializing      the next view.</li> <li>Phase-3(SV):     The new primary initializes all the replicas with log determined in phase-2 &amp; starts replication process for the      next view.    </li> </ul> <p>StartViewChange(SVC):      Upon timeout expiry one or more backups increment the view &amp; start SVC broadcast messaging.      If a replica receives a quorum of SVC messages then it sends DVC message to the next primary. DoViewChange(DVC): </p> <p>StartView(SV):      Start view message initializes the log of the backups at the start of the next view      with log &amp; commit number determined during the DVC phase.</p> <p>Safety:      Leader election is safe as quorum of hosts need to agree on the next view.      As quorum of replicas agree on the next view, requests from previous primary(if it hadn't crashed but merely      slowed down temporarily) will be ignored at least one replica which has moved to next view.      Hence previous primary will not be able to commit any request as it can't replicate to a quorum of replicas.</p> <p>Liveness:      If the chosen host is down then term gets incremented &amp; another round of SVC messaging is triggered to choose      another host as a new leader. SVC messaging should continue indefinitely until a quorum of replicas are contacted.</p> <pre><code>As hosts co-ordinate rather than compete to elect next leader FLP has minimal impact on VSR as compared to Raft. \nThe chosen host has to be done if leader election has to fail.\n</code></pre>"},{"location":"compare/compare-vsr-raft/#commit","title":"Commit","text":"<ul> <li>Leader doesn't overwrite</li> </ul>"},{"location":"compare/compare-vsr-raft/#recovery","title":"Recovery","text":"<p>Raft persists every request as it gets replicated hence the mechanism required in recovering a failed follower is minimal.  Further Leader assists the follower to recover. </p> <p>VSR doesn't use disk. In practical implementations VSR persists client request to the disk in the background.  VSR restores it's state from other hosts in the system. For safety reason it contacts a quorum of hosts which  needs to consist leader as well, to fetch the latest state.</p>"},{"location":"compare/compare-vsr-raft/#reconfiguration","title":"Reconfiguration","text":""},{"location":"compare/compare-vsr-raft/#conclusion","title":"Conclusion","text":"<p>VSR relies only a single axiomatic rule that commit of entry is safe if it is replicated on quorum of replicas. Raft follows the philosophy follow the leader, intelligence is concentrated on leader.</p> <p>State space reduction should be aim of algorithm design  Reducing the amount of mechanism involved in the algorithm is helpful for practical implementation.</p> <p>Replication:</p> <p>Leader election:</p> <p>Recovery: </p> <p>Reconfiguration:      Raft reconfiguration is very simple</p> <p>While VSR uses simple axiomatic rules for commit guarantee, Raft doesn't  Raft seems like an engineering solution coming out of industry while VSR presentation is more academic in nature.</p>"},{"location":"paxos/understand-paxos/","title":"Understanding Paxos With Three Simple Rules","text":""},{"location":"paxos/understand-paxos/#introduction","title":"Introduction","text":""},{"location":"paxos/understand-paxos/#fault-tolerant-system","title":"Fault Tolerant System","text":"<p>Consider a client server system, clients can send requests to the server. The server changes it\u2019s state as it serves the client requests. For instance, a seat reservation system reduces the number of available seats &amp; sends a confirmation to the client. The server can be abstracted as a state machine. </p> <p>A state machine(SM) starting in certain state(some number of available seats) transitions to new state(reduced number of available seats) by executing a command taken as a input(client request) while producing an output(reservation confirmation). </p> <p>If the next state of a state machine depends completely on the input being executed then it is said to be deterministic. A set of deterministic state machines will behave identically if they<sup>1</sup>: </p> <ul> <li>start in the same initial state.</li> <li>execute the same set of commands.</li> <li>excute the commands in the same sequence.</li> </ul> <p>Such a set of state machines is called a replicated state machine(RSM). A RSM can sustain some of the server(henceforth called process) failures and hence provide fault tolerance. As you can see the group of servers constituting RSM need to agree on the set &amp; order of the commands to be executed, that brings us to consensus.</p>"},{"location":"paxos/understand-paxos/#problem-definition","title":"Problem definition","text":"<p>Consensus is defined as a group of processes agreeing on a single value(i.e client request to be executed).  Consensus needs to address the following three conditions:</p> <ul> <li>Agreement: A single value is chosen.</li> <li>Validity: The value agreed upon value is actually proposed(*this is to avoid trivial solutions, all processes can simply decide a fixed value, say 42, no matter what).</li> <li>Termination: Eventually a value gets chosen.</li> </ul> <p>The first two are safety properties &amp; last one is liveness property<sup>2</sup>.</p> <p>If a group of processes are able to agree on single value(basic or single decree paxos) then the solution can be extended to agree on a sequence of values(multi paxos) which will be useful in realizing a RSM. </p>"},{"location":"paxos/understand-paxos/#environment","title":"Environment","text":"<p>An algorithm needs to consider the environment in which it is supposed to work. The processes are connected by a network &amp; communicate with each other by exchanging messages, the processes &amp; network together are referred to as a system. </p> <ul> <li> <p>Processes: </p> <ul> <li>A process can take arbitrary time to respond back to a message. </li> <li>A Process can crash &amp; restart. Since consensus is impossible if a process forgets everything after a crash,    it is assumed that processes can use stable storage to remember certain information across a crash. </li> </ul> </li> <li> <p>Network: </p> <ul> <li>Messages can get arbitrarily delayed in the network. </li> <li>Messages can get duplicated, be delivered out-of-order or get lost. </li> <li>The network is considered to be reliable in the sense that if the sending process retries then message will be delivered, eventually. </li> </ul> </li> </ul> <p>Since the processes &amp; network don't provide any guarantees w.r.t time the system is said to be asynchronous. The above conditions mean that the algorithm need to be devised without relying on time for it to be robust &amp; work correctly(ensure safety).</p> <p>Failure Detection</p> <p>A fundamental problem in an asynchronous distributed system is that it is impossible to tell whether a process has crashed or slowed down.</p> <p>Why can't failure be detected using a request-response mechanism ? The expected time to receive a reply for a ping request can't be bounded as, by definition, a process can take arbitrary time to respond back      &amp; the network transit time delay for a message is also unbounded.</p>"},{"location":"paxos/understand-paxos/#consensus","title":"Consensus","text":"<p>Acceptors &amp; proposers: The processes are divided into a set of proposers &amp; acceptors. Proposers propose to acceptors in order to choose a value.  It is possible for a single process to be an acceptor &amp; proposer but for simplicity we will consider acceptors &amp; proposers to be distinct.</p> <p>Single fixed proposer: A single proposer accepts requests, chooses one among them &amp; proposes to all the acceptors. This solution doesn't work as it can't make any progress if the fixed proposer fails.  If a new proposer is needed to step-in then it needs to do two things: </p> <ul> <li>Ensure that the old proposer doesn't affect the system adversely if it comes back(the old one din't fail but slowed down, as discussed earlier it can't be said derterministically if a process has failed).</li> <li>Reconcile the current system state &amp; accordingly propose a value to ensure safety.</li> </ul> <p>Multiple proposers: Multiple proposers accept requests &amp; propose to the acceptors. Acceptors accept only once &amp; a value is considered to be chosen if it gets accepted by a majority of the acceptors. Since any two majorities have atleast one acceptor in common &amp; acceptors accept once - only a single value can be chosen. However this solution doesn't work if:</p> <ul> <li>No majority accepts a single value amidst multiple concurrent proposals.</li> <li>An acceptor in the majority dies after a value had been chosen.</li> </ul> <p>Note that if there are 2N+1 acceptors then N+1 or more acceptors form a majority, since majority acceptance is sufficient  to choose a value, upto N acceptor failures can be sustained.</p> <p>Single vs Multiple Proposers</p> <p>Paxos assumes multiple proposers &amp; builds upon it dealing with two fundamental problems in distrbuted systems viz. concurrency &amp; failure. Concurrency is hard to reason about while failure can't be ascertained in an asynchronous system, deterministically. Moreover a proposer or an acceptor may fail at any moment(the algorithm needs to be safe).</p> <p>Replication algorithms like Raft, View Stamped Replication(VSR) &amp; Zookeeper Atomic Broadcast(ZAB) build upon single fixed proposer approach. They aren't  consensus algorithms but implement RSM.</p>"},{"location":"paxos/understand-paxos/#the-three-rules","title":"The Three Rules","text":""},{"location":"paxos/understand-paxos/#rule-one-multiple-rounds","title":"Rule One: Multiple Rounds","text":"<p>Owing to concurrency &amp; failure problems discussed in the previous section, more than one proposal may be required to achieve consensus. </p> <ul> <li>If no majority is achieved owing to multiple simultaneous proposals then a new proposal needs to be tried to achieve consensus. </li> <li>If an acceptor fails after a majority is achieved then new proposal should re-establish the majority for the same value. Accordingly acceptors should be allowed to accept more than one proposed values.  </li> </ul> <p>Proposal Numbers: Since multiple proposals are involved they need to be numbered. Each proposal carries a proposal number &amp; value being proposed.  Paxos is based on two simple invariants viz. </p> <ul> <li>Single value is proposed for a given proposal number. A proposal number can't be reused to propose a different value.</li> <li>Once consensus is achieved for a certain value then all subsequent proposals can achieve consensus only for that value. </li> </ul> <p>To meet condition one each proposer chooses proposal numbers from disjoint sets to avoid reuse across proposers &amp; a proposer can simply store the last proposal number used by itself to prevent reuse by self. For condition two total order of proposal numbers is required(notice the word subsequent), also a two phase protocol discussed below becomes imperative. </p> <p>Proposal number is constituted using two integers &lt; I,J &gt;. Each proposer is numbered uniquely &amp; this constitutes the second part \"J\" of the proposal number which will ensure that the proposal numbers are unique to the proposer &amp; doesn't get reused across proposers. For instance proposer one will use 11, 21, 31 etc, while  proposer two will use 12, 22, 32 etc &amp; so on. The first part of proposal number \"I\" is a monotonically increasing number. A proposer, at the start of each new proposal, queries a quorum of acceptors to know about all past proposals &amp; chooses a new \"I\" such that the new proposal number is greater than all the past proposals.</p> <p>Two phase protocol: At the start of each new proposal, first a query is made to atleast a majority of acceptors  to know the system state &amp; then a value is considered for current proposal. The query phase is called prepare &amp; propose phase is called accept. Restrictions are put on what value can be proposed which is discussed in the next two rules, this rule is more about multiple porposals &amp; how proposal numbers need to be chosen. </p> <p>Note that there can multiple simultaneous ongoing proposals but no two proposals should achieve consensus for different values.</p> <p>Rule One: Mutliple proposals may be required to achieve consensus</p> <p>Due to concurrency &amp; failure, more than one proposal may be needed to achieve consensus. A single value is  proposed in each proposal &amp; proposals are totally ordered(those which start later have a higher proposal number)."},{"location":"paxos/understand-paxos/#rule-two-kill-the-past","title":"Rule Two: Kill The Past","text":"<p>Consider the situation in the below diagram: An acceptor common between two proposals hasn't accepted any value<pre><code>    |&lt;---P1---&gt;|            * N1 &amp; N2: Process groups consisting of N acceptors each.\n    +------+---+------+     * Proposals P1 &amp; P2 have an acceptor \"A\" in common, P2 &gt; P1.\n    |  N1  | A |  N2  |     * \"A\" hasn't accepted a value from P1, A.value = nil.  \n    +------+---+------+     * N1 &amp; N2 don't have any other acceptor in common.\n           |&lt;---P2---&gt;|    \n</code></pre></p> <ol> <li>Acceptor \"A\" which is common between proposals P1 &amp; P2 hasn't accepted any value.</li> <li>Since acceptors in N1 are not involved in P2, it is not possible for proposal P2 to know what value  acceptors in N1 have accepted(or will accept in future &amp; achieve consenus).</li> <li>P2 can choose to propose any value, however it should ensure that a different value is not chosen by P1.</li> <li>To deal with above situation, proposer for P2 makes a prepare request &amp; extracts a promise from acceptors in N2 &amp; \"A\"  not to accept any proposal lesser than itself.</li> <li>Hence proposal P1 will not be able to achieve acceptance from a majority i.e P2 just killed P1.    Note that a quorum of acceptors (N2 + A) have moved to a higher numbered proposal hence P1 will not be able to achieve consensus.</li> </ol> <p>Note: </p> <ul> <li>There can be more than one acceptor common between two proposals.</li> <li>There could be more than one proposal before current proposal, all those proposals in which at least one common acceptor hasn't yet accepted a value will be killed by current proposal.</li> </ul> <p>Rule Two: Kill The Past</p> <p>Extract a promise from acceptors participating in the current proposal not to accept any proposal having proposal number lesser than the current one. Once a promise has been extracted from a quorum, some of the ongoing proposals(in prepare or accept phase) which have a lesser proposal number can't achieve consensus."},{"location":"paxos/understand-paxos/#rule-three-past-is-prologue","title":"Rule Three: Past Is Prologue","text":"Single Accepted Value From Previous ProposalMultiple Accepted Values From Different Past Proposals <p>Note: Make sure you read next tab after reading this one. Acceptor A which is common between proposals P1 &amp; P2 has accepted a value.<pre><code>|&lt;---P1---&gt;|            * N1 &amp; N2: Process groups consisting of N acceptors each.\n+------+---+------+     * Proposals P1 &amp; P2 have an acceptor \"A\" in common, P2 &gt; P1.\n|  N1  | A |  N2  |     * \"A\" has already accepted a value from P1, A.value = P1.value.\n+------+---+------+     * N1 &amp; N2 don't have any other acceptor in common.\n       |&lt;---P2---&gt;|\n</code></pre></p> <ol> <li>Acceptor \"A\" has accepted \"a\" as a value from P1.</li> <li>If a different value, say \"b\", were to be chosen in proposal P2 then: <ul> <li>Acceptors in N1 can complete the proposal P1 which chooses \"a\".       Note that proposal P2 doesn't has any control over the acceptors in N1 as they are not part of the proposal.      It's also possible P1 has already reached consensus.</li> <li>Proposer for P1 would deem \"a\" as chosen value &amp; proposer for P2 would deem \"b\" as chosen value.</li> <li>Ultimately, though a majority has accepted \"b\" after P2 completes, there would have been a flip from \"a\" to \"b\"       at some point in time if proposal P1 completes before P2. </li> </ul> </li> <li>To avoid above situation, before proposing a value in proposal P2, the proposer tries to learn if any of the acceptors in P2 had already       accepted a value from past proposal in the prepare phase. If so, then the previously accepted value is used for the current proposal P2.</li> <li>We are still not done, what if more than one common acceptors have accepted different values from different past proposals.      A question that arises is which value should be considered for the current proposal? Continue to the next tab for answer.</li> </ol> <p>Note: Make sure you have read previous tab before reading this one.   As per quorum property the current proposal will have one or more common acceptors in each of the past proposals.Hence, whenever a new proposal    starts it will able to know about all the past proposals(&lt; proposal-number, value-proposed &gt;) through prepare phase.  Given a past proposal &amp; current proposal, the past proposals will fall into two categories:</p> <ul> <li>At least one common acceptor hasn't accepted a value, such proposals will be killed following Rule-2.      </li> <li>One or more common acceptors has accepted a value, such proposals will be in one of these states: dead, ongoing or consensus achieved.      However, the current proposal can't know about the status of the past proposals.      Current proposal has to choose one value from the different values proposed in these past proposals preserving safety.</li> </ul> <p>Lets begin from the start. Considering the very first two proposals, two things are possible: </p> <ul> <li>P2 proposal kills P1, the value proposed in P2 &amp; P1 proposals can differ(following Rule-2). We get the sequence: \"dead-ongoing\".</li> <li>P2 chooses the same value as P1(as discussed in previous tab), both P2 &amp; P1 are ongoing. We get the sequence: \"ongoing-ongoing\".</li> </ul> <p>For the third proposal, the following below given scenarios are possible in the prepare phase when a query is made.   <pre><code>    a               b               c               d\n    ------------------------------------------------------------    \n    dead-ongoing    dead-ongoing    dead-ongoing    dead-ongoing\n       ^       ^       ^                       ^ \n\n  ^: Indicates acceptors in current proposal respond with values accepted in the past proposals when a \n     prepare request is made for current proposal.\n\n  a: The accepted values correspond to dead &amp; ongoing proposal. \n     Here value corresponding to ongoing proposal needs to be considered for current proposal \n     to preserve safety.\n\n  b: The accepted value corresponds to dead proposal. Since no acceptor participating in the current proposal \n     has accepted a value from the ongoing proposal, it will get killed. \n     So it is safe to consider the value corresponding to the dead proposal.\n\n  c: The accepted value corresponds to an ongoing proposal. \n     It's safe to choose this value for current proposal.\n\n  d: None of the accceptors in current proposal have accepted a value in past proposals. \n     All the past proposals(including already dead, aha) will get killed.\n     The current proposal is free to choose any value from the wild for the current proposal. \n</code></pre>   When a new proposal P3 starts, is it possible to get a sequence like: \"ongoing-dead\".    Yes this is possible. However, as far as the values(for safety we are concerned only with the value) are concerned it is still equivalent to \"ongoing-ongoing\".   Generalizing, it is easy to see that we need to consider a sequence like: \"ddddooooo\", dead proposals followed by ongoing proposals.   A sequence like \"ooddodd\" is equivalent to \"ooooooo\" as the dead proposals will have the same value as the ongoing proposals.</p> <p>Given a sequence like \"ddddooooo\" &amp; if more than one acceptor in current proposal had already accepted a value in past proposals, then it is safe to consider the value corresponding to the highest numbered past proposal. If the highest numbered proposal corresponds to an ongoing proposal, the sequence gets extended with another ongoing proposal. However, if highest numbered proposal cooresponds to a dead proposal then all the ongoing proposals will get killed &amp; the new sequence will have only one ongoing proposal.</p> <p>Once consensus is achieved only that value can be used in all future proposals as all future proposals will have at-least one common acceptor with   the proposal which achieved consensus. This holds true even if there are upto N failures.</p> <p>Also, note that acceptors need not have to remember all the values proposed in the past proposals, the one corresponding to the highest numbered proposal is sufficient.</p> <p>Rule Three: Past Is Prologue</p> <p>If more than one acceptor participating in the current proposal had already accepted a value from different past proposals  then use the value corresponding to the highest-numbered past proposal for the current proposal to ensure safety."},{"location":"paxos/understand-paxos/#safety-liveness","title":"Safety &amp; Liveness","text":""},{"location":"paxos/understand-paxos/#safety","title":"Safety","text":"<p>Paxos algorithm is safe to proposer &amp; acceptor failures, they may fail or become slow at any moment.   Note that acceptors in the system &amp; their total number should be known apriori.   Paxos can sustain upto N acceptor failures given 2N+1 acceptors, if more than N acceptors fail then it can't make progress as majority is needed.   However system remains safe &amp; can continue operation once the acceptors restart. Paxos is safe even if there are multiple simultaneous proposals.    Any number of proposers may fail, or a proposal can simply be abandoned in the middle. </p> <p>Moment of consensus: Consensus is achieved at the moment the last acceptor in the quorum accepts a value. At this moment neither an acceptor nor the proposer know that    consensus has been achieved. At this moment either the proposer or an acceptor may fail, however consensus can be achieved again with a new proposal but for the same value.   It is easy to reason about proposer failure. If an acceptor had failed then new majority will need atleast one acceptor from the quorum which had achieved consensus earlier - thus past consensus value makes it's way to the subsequent proposals.</p> <p>Paxos is safe to message delay, drop, duplication &amp; re-order. The sending &amp; receiving of messages is just an additional detail, safety of the Paxos algorithm is    ensured by how proposers &amp; acceptors handle the messages.</p>"},{"location":"paxos/understand-paxos/#liveness","title":"Liveness","text":"<p>Note that initially we stated that eventually a value should be decided by all correct processes.  Under certain condition this is not possible, this is not a serious limitation though &amp; can be circumvented for practical purposes.</p> <p>Rule-2 causes liveness issue, lets say an ongoing proposal is killed &amp; new one starts. But then the current proposal can get killed by a future proposal  &amp; the process can continue endlessly. Hence a value never gets chosen. The scenario is illustrated in the diagram below. There are two proposers &amp; for simplicity a single acceptor(common between the two proposals) is being shown. It keeps on rejecting  the proposals as it keeps getting higher numbered prepare requests before an accept request.</p> Paxos liveness issue due to livelock<pre><code>P1 ---&lt;1&gt;---------[1,a]----&lt;3&gt;---------------[3,a]----      &lt;n&gt;: proposal request\n        \\           \\       \\                  \\            [&lt;proposal-no&gt;, &lt;value&gt;]: accept request  \n         \\           \\       \\                  \\           x: rejected\nA1 ------&lt;1&gt;---&lt;2&gt;----x------&lt;3&gt;----x----&lt;4&gt;-----x---\n               /                   /      / \n              /                   /      / \nP2 ---------&lt;2&gt;----------------[2,b]----&lt;4&gt;----------\n</code></pre> <p>The above liveness issue is attributed to FLP result, which states that consensus is not possible in an asynchronous system even if there is  a single process failure(which occurs in the nick of time thus preventing consensus from being achieved).</p> FLP in the context of Paxos <p>Liveness issue discussed above is attributed to the famous FLP proof which asserts that consensus is impossible even with a single faulty  process in an asynchronous system. What FLP really means is that consensus is not possible sometimes under certain conditions.  Consider the below excerpts taken from the FLP paper.</p> <p>\u201cwindow of vulnerability\u201d - an interval of time during the execution of the algorithm in which the delay or inaccessibility  of a single process can cause the entire algorithm to wait indefinitely.</p> <p>the stopping of a single process at an inopportune time can cause any distributed commit protocol to fail to reach agreement. </p> <p>The theorem is proved by delaying a message at a crucial point during the execution &amp; thus steering the algorithm execution  away from consensus being achieved. The delaying of message is attributed to a faulty process - crashed or slow process.  And in an asynchronous system it's impossible to distinguish between slow process &amp; crashed process. </p> <p>This leads to the dilemma between two choices:</p> <ul> <li>whether to wait(slow process) which can lead to indefinite block if the process has actually crashed.</li> <li>proceed further(crashed process) by resetting the execution(disabling the current execution to ensure safety).      However, in this case, a future reset is imaginable &amp; the process continues endlessly.</li> </ul> <p>In the case of Paxos, Rule-2 disables previous proposals as it starts a new proposal which can itself get disabled by a future proposal  as illustrated earlier.</p> <p>Paxos suggests to elect a single distinguished proposer called leader. If the leader is given sufficient time to complete both the phases(prepare &amp; accept) &amp; the system(proposer, acceptors &amp; network) is working properly then consensus will be achieved. Note that the liveness issue doesn't disappear but it is cleverly diverted to leader election(where it prevails). Sloppy timeouts can be used so that there aren't many proposers at the same time.  </p>"},{"location":"paxos/understand-paxos/#paxos-algorithm","title":"Paxos Algorithm","text":"<p>The complete paxos algorithm is summarised below:</p> <p>Complete Paxos Protocol</p> <pre><code>Phase 1. (a) A proposer selects a proposal number n and sends a prepare\n         request with number n to a majority of acceptors.\n\n         (b) If an acceptor receives a prepare request with number n greater\n         than that of any prepare request to which it has already responded,\n         then it responds to the request with a promise not to accept any more\n         proposals numbered less than n and with the highest-numbered proposal\n         (if any) that it has accepted.  \n\nPhase 2. (a) If the proposer receives a response to its prepare requests\n         (numbered n) from a majority of acceptors, then it sends an accept\n         request to each of those acceptors for a proposal numbered n with a\n         value v, where v is the value of the highest-numbered proposal among\n         the responses, or is any value if the responses reported no proposals.\n\n         (b) If an acceptor receives an accept request for a proposal numbered\n         n, it accepts the proposal unless it has already responded to a prepare\n         request having a number greater than n.\n</code></pre> <p>Note</p> <p>The quorum in the accept phase need not necessarily be the same as the one during prepare phase.</p> <p>Since safety is guaranteed by rule-2 &amp; rule-3 during prepare phase, it doesn't matter which  acceptors are part of the quorum during accept phase. However, checkout this  potential bug.</p>"},{"location":"paxos/understand-paxos/#replicated-state-machinersm","title":"Replicated State Machine(RSM)","text":"<p>Practical systems require that a set of processes agree on a series of values instead of just one value. The decided values are added to a log  with each decision going into an index in the log &amp; since all acceptors will have the same log it is referred to as replicated log, shown below.</p> <pre><code>        *                                                   x\n        |                                                   |     \n    *   x   *       *                                       x   *   x\n    |   |   |       |                                       |   |   |\n    x   x   x   *   x                               *   *   x   x   x   *   *\n    |   |   |   |   |                               |   |   |   |   |   |   |\n  +---+---+---+---+---+                           +---+---+---+---+---+---+---+ \n  | 1 | 2 | 3 | 4 | 5 |                           | 1 | 2 | 3 | 4 | 5 | 6 | 7 |  \n  +---+---+---+---+---+                           +---+---+---+---+---+---+---+ \n                                                  &lt;--L1--&gt;|&lt;~~~~~~~~~&gt;|&lt;--L2--&gt;    \n                                                          |&lt;-------L2--------&gt;|                \n\n  Fig.a: Multiple instances of Paxos,             Fig.b:     \n  each instance chooses value for one slot.       &lt;----&gt;: Leader is established\n  Vertical lines indicate Paxos run(proposals)    &lt;~~~~&gt;: Leader has died, multiple  \n                                                          proposers are vying for leadership. \n                                                          Holes are possible, 4 is accepted \n                                                          while 3 &amp; 5 aren't accepted. \n                                                          Once L2 establishes it's leadership it will \n                                                          consolidate the log(partially accepted values) &amp; \n                                                          continue being a sole leader starting from index 6.   \n\n  x: partially accepted value\n  *: consensus achieved\n</code></pre> <p>For realizing a replicated log multiple instances of paxos can be run as shown in the above diagram on the left side.  However this is not an efficient implementation. In a more practical implementation a single proposer(called leader) can issue  just accept requests for multiple slots as shown in the above diagram on the right side.  Leader is selected through leader election &amp; leader maintains authority(single leader exists) using heart beat mechanism,  other proposers won't vie for leadership as long as they receive heart beat messages. </p> <p>committing a value: A replicated log basically provides a fault tolerant layer using which distributed applications(like db, object storage etc) can be built.  Once a value is chosen it can be used by application layer - this is called commit, note that all the values in the log need to be committed in a serial order.  Since only the leader knows consensus was achieved the commit index also needs to be communicated to the acceptors. The leader can propose values for multiple slots simultaneously, so it is possible a proposal for greater numbered index to get accepted before a proposal  for lesser numbered index is accepted i.e holes are possible in the log. In this case the greater numbered index can't be committed till values for all  lesser numbered indices are also committed.</p> <p>handling leader failure: The leader can fail while proposing values so there can be multiple indices which are partially accepted. A new leader needs to be elected,  the elected leader runs consensus(both propose &amp; accept) for all slots greater than the commit index. There can be multiple proposals for the partially accepted  slots if elected leader dies soon after being elected in the middle of proposal.</p> <p>Once a stable leader is established &amp; it resolves all the partially accepted indices. There after it can continue with normal operation wherein it would just  issue accept &amp; commit requests.</p> <p>Note that multi-paxos is note well documented, the above description is only rough sketch. Practical implementation would have to consider many  things adding/removing or increasing/decreasing the no. of acceptors in system, materializing the log to application state, application state transfer among acceptors for recovery in order to bring the acceptor to latest system state etc. </p>"},{"location":"paxos/understand-paxos/#further-reading","title":"Further Reading","text":"<p>Basic Paxos: Leslie Lamport first developed Paxos as voting algorithm which uses shared memory model. If you are still not clear with above explanation you can check  out the voting algorithm here (you will get to learn TLA+ as well):</p> <ul> <li>The Paxos algorithm or how to win a Turing Award. Part 1.</li> <li>The Paxos algorithm or how to win a Turing Award. Part 2.</li> </ul> <p>The above two videos are highly recommended(TLA+ crash course is a bonus).</p> <p>You can also check out:  Lampson's How to Build a Highly Available System Using Consensus. &amp; Paxos lecture (Raft user study)</p> <p>Multi-Paxos &amp; Reconfiguration: Two things complicate Paxos viz. need to a series of values for practical applications &amp; changing the acceptors(membership) in the system. Multi-paxos is not well documented, one can look into the formal specification of multi-paxos.  </p> <p>In distributed systems it is a given that hosts will fail ultimately.  Hence the group of acceptors need to be changed: either replace an acceptor eventually or increase/decrease the no. of acceptors in the group  to alter fault tolerance. To understand more about the same you can see  Paxos made moderately Complex &amp; Vertical Paxos. </p> <p>Distributed Lock: One practical application of paxos is to hold distributed locks in a fault tolerant manner:  Chubby service &amp; Zoo keeper.</p> <p>Practical Implementation: For challenges concerning production grade implementation you can check out Paxos made live.</p> <p>Perhaps, basic Paxos is simple to understand, it is the combination of reconfiguring a paxos group, multi-paxos, implementing a  service like distributed locking using consensus &amp; operating a production grade system is what poses challenges.  </p> <p>Replicated log: Apart from Paxos, View Stamp Replication(VSR), ZAB &amp; Raft are replication based algorithms which can be used for realizing a replicated log.  </p> <p>A replicated log is considered to be a fundamental higher level primitive for building distributed systems.  Jay Kreps has written about it here in a blog post &amp; later wrote a short book.  Corfu talks about implementing various distributed systems  like transactional KV store, databases etc &amp; AuroraDB views log as the database. </p> <p>Hence Paxos is a very fundamental algorithm for distributed systems. </p> <p>Variants: For different variants of paxos you can checkout: https://paxos.systems/variants/</p> <p>Trivia: A very nice history of consensus is published here. It would be interesting to read about how Lamport came up with Paxos algorithm &amp; it's initial reception. A subtle bug referred to here due to misinterpretation by the readers which Lamport doesn't want to disclose is, perhaps, this one.</p>"},{"location":"time/dist-sys-and-time/","title":"The Role Of Time In Distributed Systems","text":""},{"location":"time/dist-sys-and-time/#introduction","title":"Introduction","text":"<p>Time comes into picture under various contexts when dealing with distributed systems.</p> <ul> <li>System Model</li> <li>Lamport's Logical clock </li> <li>Vector Clock</li> <li>Hybrid Logical Clock</li> <li>Transactions</li> </ul>"},{"location":"time/dist-sys-and-time/#references","title":"References","text":"<ul> <li>CockroachDB - Atomic Clock</li> <li>Spanner True Time</li> <li>There Is No Now</li> <li>Vector Clock Made Easy &amp; Vector Made Hard</li> <li>Practical Uses of Synchronized Clocks in distributed Systems - Liskov</li> <li>An Overview of Clock Synchronization - Lynch </li> </ul>"},{"location":"tla-plus/tla-plus/","title":"TLA+&#58 A Design Tool For distributed System","text":""},{"location":"tla-plus/tla-plus/#introduction","title":"Introduction","text":"<p>Blueprint, Prose is not the right way Dijstra: Formula Lamport 40 year quote</p> <p>Computation model:  state: assignment of values to variables, a state is atomic wherein multiple variables are assigned values. behavior: a sequence of state transitions algorithm: set of all behavior</p>"},{"location":"tla-plus/tla-plus/#tla-distributed-systems","title":"TLA+ &amp; Distributed Systems","text":"<p>Concurrency, Failure, Non-deterministic message delivery.</p>"},{"location":"tla-plus/tla-plus/#abstraction-refinement","title":"Abstraction &amp; Refinement","text":"<p>Pablo Picasso: Nietzhe quote: </p>"},{"location":"tla-plus/tla-plus/#safety-liveness","title":"Safety &amp; Liveness","text":"<p>A program/algorithm execution can be viewed as sequence of states with values being assigned to variables in each state, conditions expressed on such executions(state transitions) for validity are called properties. Such properties validating an algorithm fall under two categories viz. safety &amp; liveness. Large program are expressed as satisfying a set of safety/liveness properties.</p> <p>Safety: Safety is defined as \"bad things shouldn't happen during the execution\". If a bad thing does happen then:  * It is irremediable &amp; can identified at a certain point during the program execution i.e it is discrete.  * Hence safety properties are supposed to hold always i.e every state transition.</p> <p>Mutual exclusion, deadlock are examples where safety property is violated. Formally, an execution is said to satisfy safety properties if every prefix of sequence of state transitions satisfies the safety properties. </p> <p>Safety conditions alone are not sufficient:  A system can be safe/correct without doing anything but then it is useless.</p> <p>Liveness: Liveness property are complementary to safety property.Liveness is defined as \"Good(useful) thing happens during the execution\".Liveness property is something which will be satisfied eventually hence it is non-discrete(unlike safety property). It is something on lines of quote by Cicero: \"While there is life, there is hope.\"</p> <p>Livelock, starvation are examples where liveness property is violated.</p> <p>mutual exclusion: * safety: no two processes can be executing the critical section at same time. * liveness: eventually, every process gets an opportunity to execute the critical section. </p> <p>Formally, every finite sequence of state transition can be suffixed with more state transitions with the possibility of satisfying liveness property. </p> <p>Safety &amp; Liveness:</p> <p>It is to be noted that any property can be expressed as conjunction of safety &amp; liveness properties.  Informally, an algorithm should remain correct along the course of its execution(safety) &amp; produce useful results eventually(liveness) without getting stuck/looping forever etc.</p> <p>(Mathematics is precise) * Precisely express safety &amp; liveness conditions.  * Vet the solution using the safety  * All possible executions are tested - algroithm's state space is explored.</p>"},{"location":"tla-plus/tla-plus/#logic-of-actions","title":"Logic of Actions","text":""},{"location":"tla-plus/tla-plus/#temporal-operators","title":"Temporal Operators","text":"<p>Safety &amp; Liveness using TLA+.</p>"},{"location":"tla-plus/tla-plus/#the-plus-in-tla","title":"The plus in TLA+","text":""},{"location":"tla-plus/tla-plus/#stuttering-fairness","title":"Stuttering &amp; Fairness","text":""},{"location":"tla-plus/tla-plus/#tlc","title":"TLC","text":""},{"location":"tla-plus/tla-plus/#tla-resources","title":"TLA+ Resources","text":"<p>Ron Presler: video https://perspectives.mvdirona.com/2014/07/challenges-in-designing-at-scale-formal-methods-in-building-robust-distributed-systems/</p>"},{"location":"vsr/analyze-vsr/","title":"Analyzing View Stamp Replication","text":""},{"location":"vsr/analyze-vsr/#replicated-state-machine","title":"Replicated State Machine","text":""},{"location":"vsr/analyze-vsr/#approach","title":"Approach","text":"<p>Lamport has very a short paper<sup>[1]</sup> on how a problem needs to be approached.  The point it makes is first define precise correctness conditions before proposing a solution.</p> <p>Defining the correctness condition independent of solution helps in formulating the solution in precise terms &amp; subsequent verification of the solution in terms of  the correctness conditions(sometimes solution is defined in terms of the correctness condition itself).</p> <p>So we will discuss this paper as problem, correctness, solution &amp; analysis in terms of correctness conditions(instead of proof).</p>"},{"location":"vsr/analyze-vsr/#problem-statement","title":"Problem Statement","text":"<p>Distributed systems consists of hosts connected through network. The algorithm needs to work considering it's environment.</p> <ul> <li>Concurrency:<ul> <li>Concurrent Requests: The RSM can receive concurrent requests from multiple clients.  </li> <li>Algorithm Concurrency: There can be concurrent operations internal to the algorithm execution itself.</li> </ul> </li> <li>Failure: <ul> <li>The hosts can crash &amp; restart at any time. </li> <li>The hosts can stall &amp; stop processing for arbitrary lengths of time. </li> <li>Hosts either crash and don't participate or are totally well behaved.    Partial correctness(like can send messages but are unable to receive messages) and malicious behavior is ruled out.</li> </ul> </li> <li>Network behavior: <ul> <li>The messages sent over the network can get arbitrarily delayed, may be lost, get duplicated or be delivered out-of-order.</li> <li>The integrity of the messages sent over the network is ensured. </li> <li>If messages are sent repeatedly, then it will be delivered eventually i.e network is reliable.</li> </ul> </li> <li>No Disk: VSR doesn't require persistent storage. However failure independence of hosts is assumed. </li> </ul> <p> Another limitation with distributed systems is that certain state of the system is global(like current leader) while each of the individual hosts have only local knowledge."},{"location":"vsr/analyze-vsr/#correctness-condition","title":"Correctness Condition","text":"<p>The execution of any algorithm can be visualized as a sequence of state transitions<sup>[2]</sup>, with each state assigning some value to the algorithm's variables. Any algorithm's correctness can be characterized using two distinct set of correctness  conditions<sup>[3]</sup> viz. </p> <ul> <li>Safety: Every step(state transition) the algorithms takes is correct, an incorrect step can't be remedied &amp;</li> <li>Liveness: It should be always be possible for the algorithm to take another step. </li> </ul> <p>If we consider mutex as an example then no two processes should be able to enter the critical section simultaneously(safetey) &amp; it should be possible for every process to eventually enter the critical section(liveness).</p> <p>The safety condition for the RSM is that the replicated log needs to satisfy two conditions<sup>[4]</sup>: </p> <ul> <li>Agreement: All the logs across hosts should contain the same set of commands/operations.</li> <li>Order: The order of the commands present in the logs need to be same.</li> </ul> <p>Liveness condition If a set of deterministic state machines begin with the same initial state &amp; the above two conditions are met, then  applying the commands in the log will result in identical state machines. Ensuring these two conditions are met with the problems  listed in the previous section is difficult.       <p>Note</p> <p>The logs may diverge under certain situations but they need to match exactly    w.r.t the commands that get applied to the state machine.</p>"},{"location":"vsr/analyze-vsr/#solution","title":"Solution","text":"<p>Primary:   Multiple clients can make requests to replicated state machine simultaneously. VSR chooses one replica as primary &amp; the rest    act as backups. All clients send requests to the primary replica. Primary assigns a monotonically increasing number(called operation number)    to the request received &amp; replicates the requests to other backup replicas.    </p> <p>Primary failure:   If the primary fails then all the replicas together select another replica as primary &amp; go through a reconciliation process called view-change.    The logs across the replicas are consolidated &amp; replicas are initialized with a new log to begin the next view ensuring the agreement &amp; order properties    w.r.t committed entries. </p> <p>Replicas are numbered &amp; get chosen as primary in round robin fashion as the system goes through a sequence of view changes.   For example in three replica system, replica 1 is primary for views {1,4,7}, replica 2 is primary for views {2,5,8} etc. </p> <p>Backup failure:   If backup fails then it goes through a recovery process to restore it's state to latest system state before it can rejoin   the rest of the replicas &amp; participate in various operations.</p> <p>Reconfiguration:   The set of replicas need to be altered. Reconfiguration is required to replace a fault host or increase/decrease no. of replicas    in the system. Replicas store an epoch number which is incremented whenever the system goes through reconfiguration.</p> <p>Network behavior:</p> <ul> <li>Out-of-order delivery &amp; duplicates:        Primary assigns monotonically increasing integer numbers to the requests received from clients.    </li> <li>loss: Primary retries mitigates message loss to some extent. In practical implementations retries are limited,              VSR provides another mechanism(GET-STATE operation) to deal with message loss.  </li> <li>delay: A replica can wait for a finite time(timeout) to receive a response from another replica.               However timeouts are only an approximate method, they are non-deterministic. We will see how this can              cause liveness issue(though it has limited impact) during primary election.                 </li> </ul> <p>Algorithm concurrency:   For instance backup might try to recover while view-change is in progress. Replicas enter into certain state   to deal with such concurrent operations to ensure safety. This also restricts the state space of the algorithm's   execution which helps in simplification of the algorithm.    <p>Quorum:</p> <p>Quorum intersection property:    If we consider a set of <code>2F + 1</code> elements, then at least one element is common between any two subsets consisting of <code>F + 1</code> elements(majority/quorum).   VSR uses this property to achieve reliability:. </p> <ul> <li>Primary replicates a request to at least a quorum of replicas before it commits the request to state machine,     commit number is incremented after a request is applied to state machine. </li> <li>When a primary fails &amp; new primary is elected, logs are gathered from a quorum of replicas for reconciliation of the     system state before next view is started. </li> <li>Similarly, when a backup fails it gathers state from a quorum of replicas to reinitialize it's state.   </li> </ul> <p>The above three conditions ensure that there is continuity w.r.t the system state i.e any committed entry is known   to at least one replica in the quorum. Since only a quorum of replicas are necessary for correct functioning    of the system, up to <code>F</code> failures can be sustained.    Thus the system achieves both reliability &amp; fault tolerance.  </p> <p> Summary: Problem-Solution Problem Solution Concurrent client requests A single primary handles all client requests. Algorithm Concurrency (e.g: recovery during reconfiguration) Algorithm states restrict certain concurrent operations which simplifies design &amp; helps in ensuring safety. Primary Failure New primary election leveraging quorum replication, system goes through reconciliation process. Backup Failure Recovery mechanism leveraging quorum replication,  failure independence assumed. Message loss, out-of-delivery, duplication, delay Algorithm needs additional mechanism to deal with these issues. In particular, delay needs timeout."},{"location":"vsr/analyze-vsr/#abstract-representation","title":"Abstract Representation","text":"<p>An abstract representation of a host in VSR system is as shown below:  Replica<pre><code>  - epoch-no:  epoch number abstracts system configuration, if replicas present in system change \n               then system goes through a epoch change.    \n  - view-no:   view number abstracts failure(primary replica changes), if a primary fails then \n               the system goes through a view change. \n  - op-no:     it's an index for a slot in the log, each slot contains one request or command. \n  - commit-no: identifies the op-no up to which the state machine has consumed the input requests \n               or commands.                 \n\n  Replicated log: \n  |&lt;=======================================time=====================================&gt;|    \n  |&lt;~~~~~~~~~~~~~e1~~~~~~~~~~~&gt;|&lt;~~~~~~~~~~~~~~~~~~~~~~~e2~~~~~~~~~~~~~~~~~~~~~~~~~~&gt;|\n  |&lt;----v1----&gt;|&lt;------v2-----&gt;|&lt;---v3--&gt;|&lt;----v4----&gt;|&lt;-------------v5-------------&gt;|\n  |&lt;a&gt;&lt;b&gt;&lt;c&gt;&lt;d&gt;|&lt;e&gt;&lt;f&gt;&lt;g&gt;&lt;h&gt;&lt;i&gt;|&lt;j&gt;&lt;k&gt;&lt;l&gt;|&lt;m&gt;&lt;n&gt;&lt;o&gt;&lt;p&gt;|&lt;q&gt;&lt;r&gt;&lt;s&gt;&lt;t&gt;&lt;u&gt;&lt;v&gt;&lt;w&gt;&lt;x&gt;&lt;y&gt;&lt;z&gt;|\n\n  en: epoch-no &lt;todo subscript&gt;\n  vn: view-no\n  &lt;.&gt;: operations or command\n\n  - replica-no:     each replica is numbered, view number is mapped to replica-no &amp; thus primary is determined. \n  - configuration:  the set of replicas in the system.\n  - status:         each replica maintains a status which corresponds to the sub-protocol that it is executing.\n\n  Note: Epoch &amp; view numbers can be discontinuous but operation number is continuous, always.\n</code></pre></p> <p>Messages exchanged among replicas are represented as <code>&lt;MESSAGE-NAME comma separated parameters&gt;</code> e.g <code>&lt;PREPARE v,m,n,k&gt;</code>. The attributes carried in various messages are shown below, view &amp; epoch parameter reflect the system state. </p> Message Attributes<pre><code>  v: view-number  k: commit-number      v: last normal view   i,j: replica number   e: epoch-number\n  l: log          n: operation-number   m: operation          x: context dependent\n                  (log index)           (client message)\n</code></pre>"},{"location":"vsr/analyze-vsr/#analysis","title":"Analysis","text":""},{"location":"vsr/analyze-vsr/#normal","title":"Normal","text":"<p>Correctness Condition, Normal Sub-Protocol (1)</p> <pre><code>  Safety:    \n    * Primary should ensure the agreement &amp; order condition i.e client requests \n      should be replicated consistently(no divergence) across all the replicas.       \n  Liveness:            \n    * Eventually the client requests should be replicated across all the replicas.\n    * Eventually all the requests should be applied to the state machines on both primary &amp; backup replicas.\n</code></pre> <ol> <li> <p>Normal Sub-Protocol</p> <ol> <li>Backups process request if their status is <code>normal</code> &amp; view matches with that in the request message. <ul> <li>If view-no is lesser then message is ignored </li> <li>if view-no is greater then replica moves to that view by doing a <code>&lt;GET-STATE&gt;</code> operation(discussed later).</li> </ul> </li> <li>Primary receives request from client &amp; adds it to it's log by incrementing       it's operation number.  </li> <li>Primary sends <code>&lt;PREPARE v,m,n,k&gt;</code> message to add an entry to the backup replica. </li> <li>Backup accepts <code>PREPARE</code> request only if operation number in the request is one more       compared to the top most entry's index in it's log.<ul> <li>Backup sends <code>&lt;PREPAREOK v,n,i&gt;</code> if it accepts the request from primary.</li> <li>If backup sees it is missing entries then it sends <code>&lt;GET-STATE v,n,i&gt;</code>.       to one of the replicas to get missing entries &amp; then sends <code>&lt;PREPAREOK&gt;</code> reply.</li> </ul> </li> <li>Primary commits requests if it receives a quorum of <code>&lt;PREPAREOK&gt;</code> replies,       primary communicates the commit index either through next <code>&lt;PREPARE&gt;</code> message or by       sending <code>&lt;COMMIT&gt;</code> messages, periodically. <code>&lt;COMMIT&gt;</code> also acts a heart beat mechanism.<ul> <li>If backup identifies missing entries when it receives <code>&lt;COMMIT&gt;</code> message,     backup will do a <code>&lt;GETSTATE&gt;</code> operation to fetch the missing entries.</li> </ul> </li> </ol> </li> </ol>"},{"location":"vsr/analyze-vsr/#view-change","title":"View Change","text":"<p>Correctness Condition, View-Change Sub-Protocol (1)</p> <pre><code>  Safety:    \n    * During view-change all the committed entries need to be preserved maintaining agreement &amp; order condition.       \n  Liveness:            \n    * View-change should complete &amp; normal processing should resume, should not be stuck in SVC/DVC messaging.\n</code></pre> <ol> <li> <p>View-Change Sub-Protocol</p> <pre><code>1. If a backup doesn't receive timely PREPARE/COMMIT messages then it increments it's\n   view-number, changes it's state to `view-change` &amp; triggers view-change \n   by broadcasting `&lt;START-VIEW-CHANGE v,i&gt;` message.\n*  When a quorum(including self) of SVC messages are received by a backup, it sends \n   `&lt;DO-VIEW-CHANGE v,l,v',n,k,i&gt;` message to the new primary(determined by \n   the incremented view-number), l corresponds to it's log in it's last normal view v'.  \n*  When the new primary receives are quorum(including self) of DVC messages it reconciles \n   the logs received in DVC messages. A new log is chosen based on: \n    * largest v', if multiple logs have same v' then one with largest n is chosen.\n    * k is chosen to be largest of such values received in the DVC messages.        \n* New primary sends `&lt;START-VIEW v,l,n,k&gt;` message to all replicas with log chosen in \n  the previous step and starts normal operation.\n* If replicas don't receive `&lt;START-VIEW&gt;` messages then backups trigger another round\n  of SVC/DVC messaging by advancing to the next view.\n</code></pre> <p>Note: </p> <ol> <li>The new primary might or might not be part of SVC messaging.</li> <li>Quorum reply for <code>&lt;START-VIEW&gt;</code> request is not necessary for new primary to begin normal processing as safety is    preserved by replication to quorum before commit can happen.</li> </ol> </li> </ol> <p>SVC Messaging</p> <p>SVC messaging is crucial for view-change as it ensures log stability before entering the DVC phase which reconciles the logs. A primary might have slowed down(not crashed) &amp; timeout occurred. SVC phase ensures that quorum of replicas agree to stop processing the requests from the primary before entering into DVC phase.</p>"},{"location":"vsr/analyze-vsr/#network-partitions","title":"Network Partitions","text":"<p>When a network partition occurs <code>PREPARE</code>/<code>COMMIT</code> messages will be lost which can trigger view changes.  There are two cases depending on which side of partition the primary is present. These scenarios need to be  analyzed as the paper discusses about primary failure only. </p> Network split with majority containing the primary View<sub>majority</sub> &lt; View<sub>minority</sub>View<sub>majority</sub> = View<sub>minority</sub>View<sub>majority</sub> &gt; View<sub>minority</sub> <p> time majority minority context t1 V<sub>majority</sub> = V V<sub>minority</sub> = V Network partition occurs t2 V<sub>majority</sub> = VStatus = normal V<sub>minority</sub> = V + 1Status = view-changePhase = SVC * Minority doesn't receive heart   beat messages &amp; moves to view-change. * It's stuck in SVC as there is no majority t3 * Both groups join &amp; view-change continues. * The group as whole moves to view v+1. * However this view-change is unnecessary as  majority was functioning normally. Network partition heals </p> <p> time majority minority context t1 V<sub>majority</sub> = V V<sub>minority</sub> = V Network partition occurs t2 V<sub>majority</sub> = VStatus = normal V<sub>minority</sub> = V + 1Status = view-changePhase = SVC * Minority doesn't receive heart   beat messages &amp; moves to view-change. * It's stuck in SVC as there is no majority t3 V<sub>majority</sub> = V + 1Status = normal V<sub>minority</sub> = V + 1Status = view-changePhase = SVC Primary crashes &amp; majority   advances to it's view. t4 * Majority receives SVC messages which it needs to ignore since it is already in same view with status as normal which can be considered to be further ahead on the timeline.   * Minority will receive PREPARE/COMMIT messages from primary. Minority can move to normal state by transferring state through GET-STATE operation. Note that primary could have added some entries in view V before proceeding to view V+1. Network partition heals. </p> <p> time majority minority context t1 V<sub>majority</sub> = V V<sub>minority</sub> = V Network partition occurs t2 V<sub>majority</sub> = VStatus = normal V<sub>minority</sub> = V + 1Status = view-changePhase = SVC * Minority doesn't receive heart   beat messages &amp; moves to view-change. * It's stuck in SVC as there is no majority t3 V<sub>majority</sub> = V + 2 V<sub>minority</sub> = V + 1 Majority goes through multiple view changes t4 * SVC messages from minority group will be ignored since majority has higher  view-no.* Minority receives PREPARE/COMMIT from the primary.        Minority can move to normal state by transferring state through GET-STATE operation.      Network Partition heals. </p> Network split with minority containing the primary <p>Note: View<sub>majority</sub> &lt; View<sub>minority</sub> doesn't seem to be possible  </p> View<sub>majority</sub> &gt; View<sub>minority</sub>View<sub>majority</sub> = View<sub>minority</sub> <p> time majority minority context t1 V<sub>majority</sub> = V V<sub>minority</sub> = V Network partition occurs t2 V<sub>majority</sub> = V + 1Status = normal V<sub>minority</sub> = VStatus = normal Minority continues receiving heart beat messages, hence view number doesn't advance t3 * Minority group receives PREPARE/COMMIT messages* As minority notices higher view number it does GET-STATE operation and thus advances it's view. Network partition heals </p> <p> time majority minority context t1 V<sub>majority</sub> = V V<sub>minority</sub> = V Network partition occurs t2 V<sub>majority</sub> = V + 1Status = normal V<sub>minority</sub> = VStatus = normal Minority continues receiving heart beat messages, hence view number doesn't advance t3 V<sub>majority</sub> = V + 1Status = normal V<sub>minority</sub> = V + 1Status = view-changePhase = SVC Primary crashes t4 * Majority receives SVC messages which it needs to ignore since it is already in same view with status as normal which can be considered to be further ahead on the timeline.   * Minority will receive PREPARE/COMMIT messages from primary.   Minority can move to normal state by transferring state through GET-STATE operation. Even though view is same state transfer should be done from last commit index since minority has gone through view-change &amp; in the process one or more operation numbers could have been reassigned to different client requests. Network partition heals </p> Results <p>GET-STATE is discussed only in the context when the replicas are in normal state.</p> <ul> <li>During network partition replicas can be in view-change state and receive PREPARE/COMMIT messages. Replicas need to move to normal state through <code>GET-STATE</code> operation.</li> <li>Even if view matches when backup receives PREPARE/COMMIT while it's in view-change,    it needs to transfer entries from it's last commit point(unlike as done during normal operation).  </li> <li>Sometimes unnecessary view-change is triggered.</li> </ul> <p>Note that SVC messaging should continue indefinitely(till network partition heals) for  the replicas in the backup to rejoin.</p>"},{"location":"vsr/analyze-vsr/#data-loss-with-get-state","title":"Data Loss with GET-STATE","text":"<p>Note that for <code>GET-STATE</code> in view-change scenario, the paper says backup replica clears up entries beyond it's commit index before sending a <code>&lt;GET-STATE&gt;</code> request. This should ring an alarm as an entry which is already committed might now be present on less than a quorum of replicas, violating the safety property.</p> <p>Imagine the following scenario: <pre><code>A system consisting of three replicas viz. r1, r2, r3.\n* r1 is the primary &amp; view is 1, r1 &amp; r2 participated in normal operation with\n  some entries getting committed by r1. \n* r1 goes offline(doesn't crash), view change for views 2 &amp; 3 are unsuccessful \n  with r2 &amp; r3 being participants.\n* r1 comes back &amp; becomes primary for view 4 with r1 &amp; r3 being participants,\n  however r3 din't receive start-view message, so it's log is same as it was in view 1.\n* r1 sends prepare message to r2 with view as 4 &amp; crashes, r2 truncates it's log before \n  sending &lt;get-state&gt; request.     \n\nEven though two replicas are available r2 &amp; r3, committed entry is lost.\n</code></pre></p>"},{"location":"vsr/analyze-vsr/#decreasing-commit-index","title":"Decreasing Commit Index","text":"<p>It is possible that primary has committed certain entries but the same has not been conveyed to the backup replicas through  PREPARE/COMMIT messaging i.e primary's commit-index is ahead of backups.  If a view-change occurs at this stage &amp; ships the log to previous primary(din't crash but slowed down) then it's commit-index can decrease.  The scenario is illustrated below. If primary crashes &amp; recovers then same behavior is observed. </p> Commit index decreases <pre><code>                          &lt;n=1,k=0&gt;           &lt;n=1,k=0&gt;\n1--&lt;a&gt;--(a)---&lt;a&gt;--------(a)--#--!!!!!!!!!!--sv----             &lt;a&gt;: request\n    \\   /      \\         /    &lt;n=1,k=1&gt;      /                  (a): response\n    \\ /        \\       /                   /                   x: entry added\n2-----x----------\\-----/-------------------sv------             #: commit\n      a           \\   /                     \\                   !!!: primary pauses, view-change  \n                  \\ /                       \\                  sv: start-view message  \n3-------------------x------------------------sv----\n                    a                        &lt;n=1,k=0&gt;\n</code></pre> <p>As a solution the state machine needs to dedupe based on the operation number(which needs to be persisted as entries get applied to state machine since VSR doesn't use disk for any of it's operation).</p>"},{"location":"vsr/analyze-vsr/#flp-vsr","title":"FLP &amp; VSR","text":"<p>FLP states that consensus is not possible in an async system even with a single faulty process under certain circumstances i.e consensus is  not possible sometimes. If a primary chosen through SVC-DVC messaging becomes faulty(crash or slows down) then the remaining replicas are in dilemma:</p> <ul> <li>is new primary slow, should they wait longer ?</li> <li>has the chosen primary crashed &amp; should they start another round of SVC-DVC messaging incrementing the view number ?   </li> </ul> <p>In my understanding, this is the manifestation of FLP in the VSR algorithm.  It seems like FLP has much lesser impact compared to RAFT or Paxos as replicas don't compete(to be next primary)  rather co-ordinate in selecting the next primary. The impact is minimal as the chosen primary has to be down.</p>"},{"location":"vsr/analyze-vsr/#recovery","title":"Recovery","text":"<p>Correctness Condition, Recovery Sub-Protocol (1)</p> <pre><code>  Safety:    \n    * Backup should recover into the latest view of the system.       \n  Liveness:            \n    * Recovery should complete in timely manner without getting stuck.\n</code></pre> <ol> <li> <p>Recovery Sub-Protocol</p> <pre><code>1. VSR doesn't use disk, failure independence is assumed. \n* Gather state from a quorum of replicas including primary\n  of the view to which the backup is recovering.\n*\n</code></pre> </li> </ol>"},{"location":"vsr/analyze-vsr/#primary-failure","title":"Primary Failure:","text":"<p>If a primary fails then it recovers into the next view after a new primary is elected.  Since recovery process requires primary among the quorum of replicas using which recovery is executed.</p>"},{"location":"vsr/analyze-vsr/#recovery-during-view-change","title":"Recovery during view-change:","text":"<p>If a backup attempts recovery during an ongoing view-change then there are two possibilities:</p> <ul> <li>Recover into view, V, prior to view-change:     If backup is able to contact </li> <li>Recovers into view, V+1, after view-change:<ul> <li>In such cases the backup has to restart recovery process.</li> </ul> </li> </ul>"},{"location":"vsr/analyze-vsr/#practical-implementation","title":"Practical Implementation","text":"<ul> <li><code>System state = App State + Log State</code></li> <li><code>App state = Checkpoints, Snapshot, Merkle Tree</code> One may check Paxos Made Live for challenges encountered with real world implementation of RSM.</li> </ul>"},{"location":"vsr/analyze-vsr/#conclusion","title":"Conclusion","text":"<p>Inference</p> <ul> <li>On paper analysis of the algorithm is extremely difficult due to concurrency, out-of-order message delivery &amp; failure occurring at any moment.    Formal verification(like TLA+ which explores the algorithm's state space) becomes imperative.  </li> <li>Following cases w.r.t view-change operation were discovered:<ul> <li><code>View-Change</code> to <code>Normal</code> state transition with replicas in <code>view-change</code> state having same or lesser view.   Even in the case of same view, state transfer needs to be done from last commit-index.</li> <li>Data loss w.r.t <code>GET-STATE</code> operation.</li> <li>Commit index going backwards. </li> </ul> </li> <li>Following cases w.r.t recovery operation were discovered:<ul> <li>VSR makes broad sketches w.r.t recovery operation, modeling can be helpful here while filling the gaps w.r.t details.</li> <li>The recovery operation might have to be restarted if it gets triggered during an ongoing view-change operation.        </li> </ul> </li> <li>Practical implementation need to consider application state transfer along with log entries transfer for    view-change &amp; recovery operations.  </li> <li>VSR pushes all disk operation to the background in order to achieve performance. This can be limited   to data replication alone &amp; explore storing certain other information on disk if it helps in achieving   algorithm simplicity.</li> <li>VSR uses algorithm states so as to disable certain operations while others are in progress. In certain cases   this a trade-off between availability &amp; simplicity of the algorithm(vertical Paxos abstract reads as    \"reconfiguration in the middle of consensus\").</li> </ul>"},{"location":"vsr/analyze-vsr/#references","title":"References","text":"<p> 1. State the Problem Before Describing the Solution 2. Safety, Liveness, and Fairness 3. Defining Liveness"},{"location":"vsr/analyze-vsr/#-title-state-transition-between-normal-or-view-change-states-for-replicas-flowchart-lr-nnormal-svcsvc-dvcdvc-n-timeoutsvc-svc-move-to-next-viewbrdvc-not-received-timeoutsvc-svc-quorum-of-svcsdvc-svc-preparecommitbrwith-higher-or-same-viewn-dvc-preparecommitbrwith-higher-or-same-viewn-dvc-start-viewn","title":"<pre><code>---\ntitle: State transition between normal or view-change states for replicas\n---\nflowchart LR\n  N(Normal) \n  SVC(SVC)\n  DVC(DVC)\n\n  N   --&gt;|Timeout|SVC\n  SVC --&gt;|\"move to next view&lt;br&gt;(DVC not received timeout)\"|SVC \n  SVC --&gt;|quorum of SVCs|DVC\n\n  SVC --&gt;|\"prepare/commit&lt;br&gt;(with higher or same view)\"|N\n  DVC --&gt;|\"prepare/commit&lt;br&gt;(with higher or same view)\"|N\n  DVC --&gt;|\"START VIEW\"|N    </code></pre>","text":"<p>``` Replicated State Machine -&gt; Replicated log -&gt; Basis for building fault tolerant System </p> <p>The operations present in replicated log when applied to state machines across replicas result in identical machines, hence it is basic primitive for realizing various fault tolerant  distributed systems like database, storage etc.</p> <p>VSR is replication algorithm which can be used to realize a replicated state machine(RSM) which consists of a distributed/replicated log consisting of same set of operations(agreement) &amp; in the same sequence(order) .  <p>Unlike Paxos VSR is structured as replication algorithm &amp; thus more aligned towards realizing a RSM.  However, VSR uses a Paxos like consensus algorithm for part of it's operation(primary election) - atomic broadcast(reliable replication) &amp; consensus are equivalent problems. <p>Replication should satisfy two conditions:   * Agreement: All logs should contain same entries.   * Order: The entries across different logs should follow same order.   <p>These fairly simple conditions are difficult to maintain in distributed system.</p> <p>VSR is replication algorithm which uses Paxos like consensus algorithm for part of it's operation - reliable broadcast &amp; consensus are equivalent problems <p>No Disk - either for consensus or for storing replicated data, assuming independent failure.   Byzantine ??</p> <p>Client &lt;-&gt; VSR &lt;-&gt; SM ```` ==============================================================================================</p>"},{"location":"vsr/vsr-tla-old/","title":"TLA+ Spec For View Stamped Replication","text":"<p>Replicas execute one of the above operations at any given time, accordingly the replicas maintain that status.  This is mainly to avoid concurrent execution(like, say, reconfiguration during normal operation) to simply the overall algorithm.  By disabling some of the operations while the algorithm is executing certain other operations there will be availability loss but it's  a trade-off made to simplify the algorithm. VSR does include some other techniques so that overall availability is not severely affected, for example  if new replicas are to be introduced, the  </p> <p>Typically specifications use multiple TLA+ functions for representing the state of a distributed system.  For example log, state, commit-no etc each indexed by replicas. This particular implementation uses a single TLA+ function named as <code>replicaState</code> which achieves better encapsulation &amp; is indexed by replicas. This spec currently implements normal &amp; view-change operations only &amp; following safety/liveness properties. </p> <p>Analysis:  Concurrent executions are hard to reason about on paper, also given that are innumerable possible executions TLA+ proves to be very useful. TLA+ spec serves as a completely tested prototype since every possible execution is tested for safety &amp; liveness properties. Implementing the spec in TLA+ was particularly helpful in identifying the following scenarios &amp; getting a better understanding of the replication protocol.</p> <ul> <li>Numerous concurrency issues, for example, collecting/resetting SVC &amp; DVC message sets during nested view changes. The spec can used as starting point for reference during        actually implementation.  </li> <li>Commit-no can go backwards during view change. Such scenario can be addressed if state machine does dedupe using the commit-no or the VSR algorithm should ensure that commit-no grows monotonically.  </li> <li>Commit no can be greater than the log index.    Lets say there are 5 replicas, after getting responses from 2 replicas primary can advance the commit number. However up to 2 replicas might be lagging &amp; might not have all the log entries up to the commit number being sent by the primary. </li> </ul>"},{"location":"vsr/vsr-tla/","title":"TLA<sup>+</sup> Spec for View Stamp Replication","text":""},{"location":"vsr/vsr-tla/#introduction","title":"Introduction","text":"<p>paper: https://pmg.csail.mit.edu/papers/vr-revisited.pdf</p> <p>View Stamp Replication is a replication algorithm for realizing replicated state machines  which is the basis for building fault tolerant distributed systems..</p> <p>You can read about the analysis of the algorithm in my earlier post which concluded that TLA+ analysis is imperative.</p> <p>Distributed systems have three problems viz. concurrency, failure &amp; network behavior as far as algorithm design is concerned.  TLA+ is an ideal tool for modeling concurrent systems &amp; TLA+ provides some constructs to model  network behavior(specifically, out-of-order delivery which also means delay, implicitly).</p> <p>Failure &amp; message loss/duplication need to be implemented in the spec, explicitly.</p> <p>Note: Failure is fairly easy to reason about on paper as compared to concurrency &amp; non-deterministic message delivery as  innumerable different executions are possible.</p>"},{"location":"vsr/vsr-tla/#implementation","title":"Implementation","text":"<p>Please go through the post mentioned above before proceeding further, at least  go through the sections: solution, abstract representation &amp; analysis.</p> <p>Basically the system consists of replicas exchanging messages, so the spec is defined using: </p> <ul> <li>replicaState: TLA+ function to represent replicas. A couple of helper functions named <code>update*</code> are provided to manipulate replicaState.                 Generally, replicas are represented using multiple TLA+ constructs which compromises on encapsulation.</li> <li>messages: TLA+ record to represent the messages passed between replicas.</li> </ul> <p>The spec doesn't model client &amp; state machine interactions in order to keep the model size small. Also, the model considers only log based implementation i.e application state is ignored.</p> <p>Currently, the spec implements Normal &amp; View-Change sub-protocols only.  Spec: https://github.com/sunilva/tlaplus-specs/blob/main/ViewStampReplication.tla</p> <p>Limitations</p> <ul> <li>GET-STATE is not implemented. </li> <li>Message loss(network partition) is not implemented.</li> </ul>"},{"location":"vsr/vsr-tla/#testing","title":"Testing","text":"<p>The spec has been verified with following model configuration:</p> request replicaCount viewChangeTriggerLimit state space size {1,2,3} 3 0 3M {1,2} 3 1 1M {1,2,3} 3 1 205M {1,2} 3 2 44M <p>Notes:</p> <ul> <li>NIL is a model value.</li> <li>Uncheck deadlock from the model definition.</li> </ul> <p>Tip</p> <ul> <li>Run the model without checking any safety &amp; liveness properties, this will give an idea about      the size of the state space. </li> <li>Once you know state space size check for safety properties followed by liveness properties.  </li> <li>Checkout https://vimeo.com/264959035 for additional tips.</li> </ul>"},{"location":"vsr/vsr-tla/#results","title":"Results","text":"<ul> <li> <p>Concurrent executions are hard to reason about on paper. TLA+ proves to be very useful given that there will be innumerable possible executions.    TLA+ spec serves as a completely tested prototype since every possible execution(are messages delivered in every possible order?)    is tested for safety &amp; liveness properties.</p> </li> <li> <p>Sometimes the protocol authors make broad sketches, the gaps can be filled with confidence only through modelling.</p> </li> <li> <p>Testing the algorithm solution for safety &amp; liveness properties helps in better algorithm design since the    solution is tested for correctness not for coverage as is usually done with unit &amp; integration testing.</p> </li> </ul> <p>It needs to be mentioned that TLA+ comes with at least two challenges:   </p> <ul> <li> <p>Modeling is only an approximate representation of the actual system, capturing the essential problems of the actual system   into the model is challenging.    For instance VSR paper is discussed using logs(system state is represented as log &amp; messages carry entire logs) which is not practical.    In practical implementation, system state is composed of application state(commands in log applied to state machine) plus commands in the log,    log is pruned periodically. In reality, application state and log entries need to be exchanged while dealing with failure.  </p> </li> <li> <p>Model size needs to be limited as the state space grows exponentially. This creates additional tension during modeling.</p> </li> </ul>"}]}